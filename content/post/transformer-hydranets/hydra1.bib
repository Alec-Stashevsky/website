@misc{chen2022understanding,
      title={Towards Understanding Mixture of Experts in Deep Learning},
      author={Zixiang Chen and Yihe Deng and Yue Wu and Quanquan Gu and Yuanzhi Li},
      year={2022},
      eprint={2208.02813},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@BOOK{Hesiod1966-qd,
  title     = "Theogony",
  author    = "{Hesiod}",
  publisher = "Oxford University Press Reprints distributed by Sa",
  series    = "Oxford University Press academic monograph reprints",
  month     =  nov,
  year      =  1966,
  address   = "London, England"
}

@BOOK{Grimal1996-of,
  title     = "The dictionary of classical mythology",
  author    = "Grimal, Pierre",
  publisher = "Blackwell",
  month     =  jul,
  year      =  1996,
  address   = "London, England"
}

@Article{Caruana1997,
author={Caruana, Rich},
title={Multitask Learning},
journal={Machine Learning},
year={1997},
month={Jul},
day={01},
volume={28},
number={1},
pages={41-75},
abstract={Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
issn={1573-0565},
doi={10.1023/A:1007379606734},
url={https://doi.org/10.1023/A:1007379606734}
}

@InProceedings{10.1007/3-540-44581-1_15,
author="Bartlett, Peter L.
and Mendelson, Shahar",
editor="Helmbold, David
and Williamson, Bob",
title="Rademacher and Gaussian Complexities: Risk Bounds and Structural Results",
booktitle="Computational Learning Theory",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="224--240",
abstract="We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes.We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.",
isbn="978-3-540-44581-4"
}

@misc{ruder2017overview,
      title={An Overview of Multi-Task Learning in Deep Neural Networks},
      author={Sebastian Ruder},
      year={2017},
      eprint={1706.05098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
