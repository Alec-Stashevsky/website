---
title: "Topology and Geometry Through Graph ML"
author: "Alec Stashevsky"
date: 2022-11-26
categories: ["Mathematics", "Graph ML", "Deep Learning"]
tags: ["Mathematics", "Graph ML", "Deep Learning"]
bibliography: [topology-vs-geometry.bib]
link-citations: true
links:
image:
  preview_only: false
  focal_point: "Smart"
summary: Diving deeper into mathematics through my journey with industrial Graph ML application.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, dev = "svg")
library(ggplot2)
library(network)
library(sna)
library(kableExtra)
library(AlecWebsiteThmr)
library(GGally)
library(plotly)
library(rgdal)
library(data.table)
set.seed(32)
```

```{css Table CSS, echo=FALSE}
table {
   overflow-x: hidden;
}

table > thead > tr > th {
    vertical-align: bottom;
    border-bottom: 2px solid rgb(232, 215, 195);
}

table > thead > tr > th, table > tbody > tr > th, table > tfoot > tr > th, table > thead > tr > td, table > tbody > tr > td, table > tfoot > tr > td {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-top: 1px solid rgb(232, 215, 195);
}

tbody > tr > td, table > tbody > tr > th {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-bottom: 1px solid rgb(232, 215, 195);
}


table > tfoot > tr > td {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-top: none;
}



table > tbody > tr:nth-child(2n+1) > td, table > tbody > tr:nth-child(2n+1) > th {
    background-color: rgb(255, 248, 227);
}

# Hover
table > tbody > tr:nth-child(2n+1):hover > td, table > tbody > tr:nth-child(2n+1):hover > th {
    background-color: rgb(232, 215, 195);
}

table > tbody > tr:hover > td, table > tbody > tr:hover > th {
    background-color: rgb(232, 215, 195);
}
```

```{css References CSS, echo=FALSE}
#references {
    text-indent: -2em;
    margin-left: 2em;
}
```

> There is no royal road to geometry.
>
> [--- Euclid, *on account of Proclus of Athens*]{style="float:right"}
>

<br>

This was Euclid's response when asked by Ptolemy if no shorter road to geometry existed than through his [*Elements.*](http://aleph0.clarku.edu/~djoyce/elements/elements.html)^[*See* @Morrow1992-MORPAC-8 and @2017euclid.]

<br>

For the past year I've been working with a team of folks at [Fetch](https://fetch.com/) to develop and productionalize a machine learning based document understanding pipeline powering the core of the Fetch Rewards app. At Fetch, we reward our users for snapping pictures of their receipts in real-time, and we do the heavy lifting with this pipeline.

During this process, one of the things I've spent more time trying to do is understand unstructured data (such as images of receipts and the text which appears on them) in more elegant ways. This led me to a new and exciting place flourishing within the ML community: Graph Machine Learning (Graph ML). 

This post is about some of the things I've learned participating in the Graph ML and Geometric Deep Learning communities. This post is also about some of the things I've had to forget when applying Graph ML in large-scale industrial settings. 

<br>

## Graphs Are All Around Us 

When first learning about graphs, one usually encounters something like the following: 

```{r basic graph, echo=FALSE, message=FALSE, warning=FALSE}
# Create static graph
net <- structure(
  list(
    mel = list(
      list(inl = 1L, outl = 2L, atl = list(na = FALSE)),
      list(inl = 1L, outl = 3L, atl = list(na = FALSE)), 
      list(inl = 2L, outl = 3L, atl = list(na = FALSE)),
      list(inl = 3L, outl = 5L, atl = list(na = FALSE)),
      list(inl = 4L, outl = 5L, atl = list(na = FALSE))
      ),
    gal = list(n = 5, mnext = 6L, directed = FALSE,
               hyper = FALSE, loops = FALSE,
               multiple = FALSE, bipartite = FALSE
               ),
    val = list(list(na = FALSE, vertex.names = 1L, color = "type1"),
               list(na = FALSE, vertex.names = 2L, color = "type2"), 
               list(na = FALSE, vertex.names = 3L, color = "type1"),
               list(na = FALSE, vertex.names = 4L, color = "type2"),
               list(na = FALSE, vertex.names = 5L, color = "type1")
               ),
    iel = list(2:1, 3L, 4L, 5L, integer(0)),
    oel = list(integer(0), 1L, 2:3, integer(0), 4:5)
    ),
  class = "network")

# Plot the graph
ggnet2(
  net,
  color = rep(c(get_primary_color()), 5),
  edge.size = 1,
  edge.color = "#8f7346",
  label = TRUE,
  label.color = "white",
  label.alpha = 0.7
  ) + 
  theme_AlecWebsite() +
  labs(x = "", y = "")
```

A collection of *nodes* or *vertices* represented as dark green circles connected by *edges* or *links* which indicate some pairwise relationship between two nodes.

Graphs are incredibly simple yet flexible data structures. There is no notion of left, right, up, or down. Nodes and edges and can be added or omitted as we please. We can also move, bend, and reorder the nodes without changing the underlying relational structure of the graph. That is, without changing the fundamental nature of the graph itself. For example, below I've moved some of the nodes of the graph around and switched node 3 and node 5.

```{r basic graph2, echo=FALSE, message=FALSE, warning=FALSE}
# Create static graph
net2 <- structure(
  list(
    mel = list(
      list(inl = 1L, outl = 2L, atl = list(na = FALSE)),
      list(inl = 1L, outl = 3L, atl = list(na = FALSE)), 
      list(inl = 2L, outl = 3L, atl = list(na = FALSE)),
      list(inl = 3L, outl = 5L, atl = list(na = FALSE)),
      list(inl = 4L, outl = 5L, atl = list(na = FALSE))
      ),
    gal = list(n = 5, mnext = 6L, directed = FALSE,
               hyper = FALSE, loops = FALSE,
               multiple = FALSE, bipartite = FALSE
               ),
    val = list(list(na = FALSE, vertex.names = 1L, color = "type1"),
               list(na = FALSE, vertex.names = 2L, color = "type2"), 
               list(na = FALSE, vertex.names = 5L, color = "type1"),
               list(na = FALSE, vertex.names = 4L, color = "type2"),
               list(na = FALSE, vertex.names = 3L, color = "type1")
               ),
    iel = list(2:1, 3L, 4L, 5L, integer(0)),
    oel = list(integer(0), 1L, 2:3, integer(0), 4:5)
    ),
  class = "network")

# Plot the graph
ggnet2(
  net2,
  mode = "circle",
  color = rep(c(get_primary_color()), 5),
  edge.size = 1,
  edge.color = "#8f7346",
  label = TRUE,
  label.color = "white",
  label.alpha = 0.7
  ) + 
  theme_AlecWebsite() +
  labs(x = "", y = "")
```

Despite these modifications, the graph describes the same type of basic relational structure. We refer to this underlying structure as the graph's *connectivity* or the *topology* of the graph. The graph's topology is agnostic of where we position these nodes in space, or what we name individual nodes. 

This also means graphs exist outside of Euclidean space where we have explicit notions of distance and direction. I have no way of telling if node 3 is closer to node 4 or node 5 even though they appear father apart in the two examples above. Graphs like these, in their most basic form, do not model Euclid's geometry. Perhaps there is a shorter road after all?


### What Happens When We See Them

One of the reasons Graph ML is such an exciting space is just how abundant graphs are. Graph ML is used to model molecules (link), where atoms are represented as nodes and their chemical bonds represented as edges. We also see social networks where user-profiles are nodes and their friendships, followers, or interactions form edges; financial networks with bank accounts as nodes and their transactions as edges of varying amounts; and even more recently, documents.

Where graphs can be seen, Graph ML is there to teach machines how to make conclusions about them and new unseen things we might later encounter. Graph ML is being used to discover new therapeutic drugs (link), prevent the spread of misinformation in social networks (link to siraj) (and probably also help it too), identify bank fraud (capitalOne), and extract information from documents (link to graph paper).

As data scientists and machine learners, our main goal is to make sense of the world around us. We do some of that by building models in and through which our understanding may grow. When we begin to see graph structure and its natural projections onto the world, we unlock a deeper and more organic connection by which to explain and infer phenomena. One of the ways graphs unlock this connection is through a blending of topological and geometric perspective.

<br>

## Adding Geometry to Graphs

For machines to learn on data, be it graph-structured or not, we must represent the data in a way computers can digest. That is to say, we must encode the data into some *n*-dimensional Euclidean vector space, where individual data points can be represented as vectors within that space.

For example, consider GPS telemetry data used to monitor the movement of a newly released California Condor.^[Picture provided by James Sheppard. *See* https://blogs.biomedcentral.com/bmcseriesblog/2015/09/02/behind-image-3d-home-range-california-condor/.] 

![](condor_3d_map.jpeg)

The data used to make that visualization might look something like this:

```{r encoded condor data, echo=FALSE, message=FALSE, warning=FALSE}
# Load data
encoded_condor_data <- readRDS("~/Git/website/content/post/topology-vs-geometry/encoded_condor_telemetry.RDs")

# Plot table
knitr::kable(
   encoded_condor_data,
   format = "html",
   row.names = FALSE,
   align = c("l", "c", "c", "c", "c"),
   escape = FALSE
   ) %>% 
   add_header_above(
      c("Condor GPS Telemetry Data" = 5),
      align = "c",
      escape = TRUE,
      line = FALSE,
      extra_css = "background-color: rgb(232, 215, 195); font-size: 1rem; padding: 0.15rem 0;"
      ) %>%
  footnote(
      general_title = "Notes:",
      title_format = "bold",
      general =
        "--- The Time column is normalized from 0 to 1 which represent the start and end of the tracking period, respectively. <br>
         --- Each condor has an identification number. Condor 190 is  <a href='https://www.ventanaws.org/condor190.html'>Redwood Queen</a> and Condor 1031 is one of her offspring,  <a href='https://www.ventanaws.org/condor1031.html'>Iniko</a>. Redwood Queen and Iniko are real California Condors which are part of the free-ranging Central California flock! They have an amazing <a href='https://www.wildtones.com/birding-info/the-story-of-condor-chick-iniko'>story</a>.",
      escape = FALSE
  )
```


We want to represent each row in the table as a column vector.^[If we have data that is not represented with numbers, than there are [feature encoding](add link) processes which are used to turn categorical or lexical data, for example, into real-numbers such that they can still be digested by the computer as numeric vectors.] 
However, the Latitude and Longitude coordinate system is part of a fundamentally different type of geometry, a non-euclidean one native to the 3-dimensional ellipsoidal shape of the Earth. We can convert this non-euclidean space to a 2-d euclidean space in the traditional Cartesian coordinate system by using the [Universal Transverse Mercator (UTM)](https://openpress.usask.ca/introgeomatics/chapter/cartesianprojected-coordinate-systems-utm/) map projection.^[Map projections are not perfect, and some distortion will occur depending on the reference point of the mapping.] The UTM projection gives us an $(easting, northing)$ point in this 2-d space for each point in $(lat, long)$ space. After we have performed the mapping all of our data can be represented in a 5-d euclidean vector space where each vector fully describes a row in our data.

$$ \begin{bmatrix} Time \\ Condor \\ Easting \\ Northing \\ Altitude \end{bmatrix} \in \mathbb{R}^5  $$

This process introduces a consistent *geometry* to our data. That is, we can now understand the space our data exists within through its shape, size, distance, and relative position. Below, I visualize the Condor GPS Telemetry vector space, using some liberties to represent the *Time* and *Condor* dimensions as hue and point-shape, respectively.

```{r condor vector space, echo=FALSE, message=FALSE, warning=FALSE}
# Convert condor column to factor for plotly
encoded_condor_data$Condor <- as.factor(encoded_condor_data$Condor)
setDT(encoded_condor_data)

# Convert Lat, Long to UTM
coord_latlon <- SpatialPoints(
  cbind(encoded_condor_data$Longitude, encoded_condor_data$Latitude),
  proj4string = CRS("+proj=longlat")
  )

# Create data table and convert from meters to kilometers
coord_UTM <- data.table(spTransform(coord_latlon, CRS("+init=epsg:32748"))@coords) / 1000
setnames(coord_UTM, old = c("coords.x1", "coords.x2"), new = c("Easting", "Northing"))

# Combine tables 
condor_vector_space <- cbind(
  encoded_condor_data[, .(Time, Condor)],
  coord_UTM,
  encoded_condor_data[, .(Altitude)])


# Plot condor vector-space
fig <- plot_ly(
  condor_vector_space,
  type = 'scatter3d',
  mode = 'markers',
  x = ~Easting,
  y = ~Northing,
  z = ~Altitude,
  color = ~Time,
  colors = c("#bfd3e6", "#4d004b"),
  symbol = ~Condor,
  symbols = c("diamond", "circle")
  ) %>% layout(
    legend = list(title=list(text='Condor')),
    scene = list(
      xaxis = list(
        range = c(4730, 4731.5),
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        ),
      yaxis = list(
        range = c(24797, 24798.5),
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        ),
      zaxis = list(
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        )
    ),
    paper_bgcolor="#fff8e3",
    plot_bgcolor="#fff8e3"
  )

fig
```

Embedding our data into a vector space allows us to leverage Euclid's *Elements* and everything built upon them. We can compare how similar two vectors are by using methods like [cosine similarity](https://towardsdatascience.com/understanding-cosine-similarity-and-its-application-fd42f585296a) or feed these vectors into machine learning models as *features* to learn upon.

When our data has an inherent graph structure, the process is no different. We simply associate feature vectors with their given nodes or edges. Let's take our first graph from this post and re-imagine it as a representation of a small social network where nodes are user-profiles and edges indicate friendships between them. 

![](~/Git/website/content/post/topology-vs-geometry/basic_graph-1-node-features.svg)

For each node we have added a feature vector which holds some information about those user-profiles such as how long they have been on the platform $x_1$, when they last logged on $x_2$, their geographic region $x_3$, and so on. 

Assigning feature vectors to the graph overlays a Euclidean geometry upon it. We can no longer move, bend, or switch nodes on the graph because each node now exists at a certain point in our feature-vector space and to move that point would change its underlying meaning.

<br>

## The Intersection of Topology and Geometry

Graphs allow us to represent individual features with additional relationships and structure. Introducing a flexible topological structure in which Euclidean geometry can be embedded can help our models understand and generalize better.

Relational data is the most widely used type of data across industry, yet it is not often thought about in terms of its inherent graph structure.^[cite] Consider a simple SQL database representation of our small social network above which contains two tables related through User IDs.

![](~/Git/website/content/post/topology-vs-geometry/relational_db_example.png)

Each row in the blue user table holds information about a node in our graph while each row in the friendship table (green) represents an edge between the user who made the friend request and the user whom accepted. Each user can have many friendships (shown by colored lines). In this sense, all relational databases can be seen as graphs which allow for complex connections through primary and foreign key relations like the one-to-many relationship we see above.

The intuition behind adding this friendship-structure is that we can learn more about a user based on whom they are friends with and the attributes of those friends themselves. An extremely common modelling task on such data is predicting [user churn](https://medium.com/@malvikat/a-complete-guide-to-customer-churn-rate-fa6ec09d96d). We want to know which users are likely to drop off the platform and perhaps intervene. A traditional tabular machine learning model might ingest the features of our user table and learn to recognize patterns about certain types of users and their behavior that make them likely to attrite. This may work relatively well, however, it ignores the underlying *topology of friendship* such users find themselves in. Consequently, we lose the ability to answer questions like: if a user's friends churn, are they more likely to churn themselves?

Graphs unlock this capability by incorporating topological structure as an inductive bias rather than as an additional feature. In other words, acknowledging the inherent topology of our data helps models better understand *how to learn* rather than expanding *what to learn*. @cvitkovic2020supervised shows this is especially true when many tables and foreign key relationships belong to a relational database.





## Notes

- images as graphs, molecules as graphs. 

-- “the shortest path between 2 points isn’t necessarily a straight line”. end with this in reference to Euclid's quote.

https://opendatascience.com/machine-learning-with-graphs-going-beyond-tabular-data/

- graph embeddings can then be treated as the inputs to a data science or machine learning model, just as any other embedding from the traditional, tabular-based approach. The difference is that they have captured, for individual rows of the model, additional information about the relationships between the individual data points.

-- Interestingly, as one can see from the RDB schema diagrams of the datasets in Supplementary Section D, the more tables and foreign key relationships an RDB has, the better GNN-based methods perform on it, with the KDD Cup 2014 dataset being the least “relational” of the datasets and the one on which GNNs seem to offer no benefit. @cvitkovic2020supervised

GNN models can be straightforwardly combined with, e.g., pretrained
Transformer representations of TEXT inputs in the KDD Cup 2014 dataset, and trained end-to-end. @cvitkovic2020supervised

-- other popular architectures like CNN and Transformers also enforce a certain topology over the data. It's not that other model architectures don't take into account topology whatsoever. But these architectures impose a strict regular topology. Graph models and methods in the broader geometric deeper learning space purposefully adapt and model a broader range of more complex topologies.

## Whats Forgotten
-- once brought into the fold, it can be hard to stop seeing graphs everywhere you look. While pretty much everything is a graph, or can be moleded into one, sometimes acknowledging that connection explicitly isn't useful. Enforcing a regular topology help not distract the model when there is important information in the topology of the data. For example, take the image graphed from above, there is no intering local/ neighborhood information within that sturcture that can help learn about how that image is different from another structured collection of pixels. 

-- in document AI, there has been recent applicatin of Graphs, which attempt to use them to uncover a *semantic* topology. 

And that's exactly why it took me getting into Graph ML to learn about these two concepts



<!-- Bring this back at the end of post -->
## Teaching Machines to Comprehend Documents
*Document Understanding* and *Document AI* are fields which use automated methods (specifically artificial intelligence in the case of the latter) to comprehend documents, such as scanned invoices, emails, legal filings(link?), and in our case: receipts. Document Understanding is a broad field which touches a wide range of sub fields within AI: Computer Vision (CV), Natural Language Processing (NLP), and even Graph Machine Learning (Graph ML) (links?).           


Industrial application of document AI is equally broad and includes:^[*See* https://huggingface.co/blog/document-ai.]

* Identifying types of document (image classification)
* Make document text searchable (optical character recognition)
* Extracting and identifying important peices of text from a document (key information extraction)
* Answering specific questions about documents (document question answering)

Heaps of papers sitting unprocessed can now turn into rich harvests. Through Fetch, even your crumpled receipts are worth something!

- Talk about FormNet and use that to represent docuements as graphs.
<!-- End Move Block -->



- End with how you've learning about topology / geometry and larger understanding of the world


### Notes

- Features may be close together in euclidean space (cosine similarity etc)
- May also have topological, or structural similarities that can also be leveraged
- Graphs can let us learn from both the feature and their structure/connectivity/neighborhood



<br>



## References
