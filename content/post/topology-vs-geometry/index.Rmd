---
title: "Topology and Geometry Through Graph ML"
author: "Alec Stashevsky"
date: 2022-12-07
categories: ["Mathematics", "Graph ML", "Deep Learning"]
tags: ["Mathematics", "Graph ML", "Deep Learning"]
bibliography: [topology-vs-geometry.bib]
link-citations: true
links:
image:
  preview_only: false
  focal_point: "Smart"
summary: Diving deeper into mathematics through my journey with industrial Graph ML application.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, dev = "svg")
library(ggplot2)
library(network)
library(sna)
library(kableExtra)
library(AlecWebsiteThmr)
library(GGally)
library(plotly)
library(rgdal)
library(data.table)
set.seed(32)
```

```{css Table CSS, echo=FALSE}
table {
   overflow-x: hidden;
}

table > thead > tr > th {
    vertical-align: bottom;
    border-bottom: 2px solid rgb(232, 215, 195);
}

table > thead > tr > th, table > tbody > tr > th, table > tfoot > tr > th, table > thead > tr > td, table > tbody > tr > td, table > tfoot > tr > td {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-top: 1px solid rgb(232, 215, 195);
}

tbody > tr > td, table > tbody > tr > th {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-bottom: 1px solid rgb(232, 215, 195);
}


table > tfoot > tr > td {
    padding: 8px;
    line-height: 1.43;
    vertical-align: top;
    border-top: none;
}



table > tbody > tr:nth-child(2n+1) > td, table > tbody > tr:nth-child(2n+1) > th {
    background-color: rgb(255, 248, 227);
}

# Hover
table > tbody > tr:nth-child(2n+1):hover > td, table > tbody > tr:nth-child(2n+1):hover > th {
    background-color: rgb(232, 215, 195);
}

table > tbody > tr:hover > td, table > tbody > tr:hover > th {
    background-color: rgb(232, 215, 195);
}
```

```{css References CSS, echo=FALSE}
#references {
    text-indent: -2em;
    margin-left: 2em;
}
```

> There is no royal road to geometry.
>
> [--- Euclid, *on account of Proclus of Athens*]{style="float:right"}
>

<br>

This was Euclid's response when asked by Ptolemy if no shorter road to geometry existed than through his [*Elements.*](http://aleph0.clarku.edu/~djoyce/elements/elements.html)^[*See* @Morrow1992-MORPAC-8 and @2017euclid.]

<br>

For the past year I've been working with a team of folks at [Fetch](https://fetch.com/) to develop and productionalize a machine learning based document understanding pipeline powering the core of the Fetch app. At Fetch, we reward our users for snapping pictures of their receipts in real-time, and we do the heavy lifting with this pipeline.

During this process, one of the things I've spent more time trying to do is understand unstructured data (such as images of receipts and the language which appears on them) in more elegant ways. This led me to a new and exciting place flourishing within the ML community: Graph Machine Learning (Graph ML). 

This post is about some of the things I've learned participating in the Graph ML and Geometric Deep Learning communities. This post is also about some of the things I've had to forget when applying Graph ML in large-scale industrial settings. 

<br>

## Graphs Are All Around Us 

When first learning about graphs, one usually encounters something like the following: 

```{r basic graph, echo=FALSE, message=FALSE, warning=FALSE}
# Create static graph
net <- structure(
  list(
    mel = list(
      list(inl = 1L, outl = 2L, atl = list(na = FALSE)),
      list(inl = 1L, outl = 3L, atl = list(na = FALSE)), 
      list(inl = 2L, outl = 3L, atl = list(na = FALSE)),
      list(inl = 3L, outl = 5L, atl = list(na = FALSE)),
      list(inl = 4L, outl = 5L, atl = list(na = FALSE))
      ),
    gal = list(n = 5, mnext = 6L, directed = FALSE,
               hyper = FALSE, loops = FALSE,
               multiple = FALSE, bipartite = FALSE
               ),
    val = list(list(na = FALSE, vertex.names = 1L, color = "type1"),
               list(na = FALSE, vertex.names = 2L, color = "type2"), 
               list(na = FALSE, vertex.names = 3L, color = "type1"),
               list(na = FALSE, vertex.names = 4L, color = "type2"),
               list(na = FALSE, vertex.names = 5L, color = "type1")
               ),
    iel = list(2:1, 3L, 4L, 5L, integer(0)),
    oel = list(integer(0), 1L, 2:3, integer(0), 4:5)
    ),
  class = "network")

# Plot the graph
ggnet2(
  net,
  color = rep(c(get_primary_color()), 5),
  edge.size = 1,
  edge.color = "#8f7346",
  label = TRUE,
  label.color = "white",
  label.alpha = 0.7
  ) + 
  theme_AlecWebsite() +
  labs(x = "", y = "")
```

A collection of *nodes* or *vertices* represented as dark green circles connected by *edges* or *links* which indicate some pairwise relationship between two nodes.

Graphs are incredibly simple yet flexible data structures. There is no notion of left, right, up, or down. Nodes and edges and can be added or omitted as we please. We can also move, bend, and reorder the nodes without changing the underlying relational structure of the graph. That is, without changing the fundamental nature of the graph itself. For example, below I've moved some of the nodes of the graph around and switched node 3 and node 5.

```{r basic graph2, echo=FALSE, message=FALSE, warning=FALSE}
# Create static graph
net2 <- structure(
  list(
    mel = list(
      list(inl = 1L, outl = 2L, atl = list(na = FALSE)),
      list(inl = 1L, outl = 3L, atl = list(na = FALSE)), 
      list(inl = 2L, outl = 3L, atl = list(na = FALSE)),
      list(inl = 3L, outl = 5L, atl = list(na = FALSE)),
      list(inl = 4L, outl = 5L, atl = list(na = FALSE))
      ),
    gal = list(n = 5, mnext = 6L, directed = FALSE,
               hyper = FALSE, loops = FALSE,
               multiple = FALSE, bipartite = FALSE
               ),
    val = list(list(na = FALSE, vertex.names = 1L, color = "type1"),
               list(na = FALSE, vertex.names = 2L, color = "type2"), 
               list(na = FALSE, vertex.names = 5L, color = "type1"),
               list(na = FALSE, vertex.names = 4L, color = "type2"),
               list(na = FALSE, vertex.names = 3L, color = "type1")
               ),
    iel = list(2:1, 3L, 4L, 5L, integer(0)),
    oel = list(integer(0), 1L, 2:3, integer(0), 4:5)
    ),
  class = "network")

# Plot the graph
ggnet2(
  net2,
  mode = "circle",
  color = rep(c(get_primary_color()), 5),
  edge.size = 1,
  edge.color = "#8f7346",
  label = TRUE,
  label.color = "white",
  label.alpha = 0.7
  ) + 
  theme_AlecWebsite() +
  labs(x = "", y = "")
```

Despite these modifications, the graph describes the same basic relational structure. We refer to this underlying structure as the graph's *connectivity* or the *topology* of the graph. The graph's topology is agnostic of where we position these nodes in space, or what we name individual nodes. 

This also means graphs exist outside of Euclidean space where we have explicit notions of distance and direction. I have no way of telling if node 3 is closer to node 4 or node 5 even though they appear father apart in the two examples above. Graphs like these, in their most basic form, do not model Euclid's geometry. Perhaps there is a shorter road after all?


### What Happens When We See Them

One of the reasons Graph ML is such an exciting space is just how abundant graphs are. Graph ML is being used to model particle physics, where particles are represented as nodes and their physio-chemical interactions represented as edges.^[*See* @Shlomi_2021.] We also see social networks where user-profiles are nodes and their friendships, followers, or interactions form edges; financial networks with bank accounts as nodes and their transactions as edges of varying amounts; and even more recently, documents.

Where graphs can be seen, Graph ML is there to teach machines how to make conclusions about them and new unseen things we might later encounter. Graph ML is being used to [discover new therapeutic drugs](https://towardsdatascience.com/drug-discovery-with-graph-neural-networks-part-1-1011713185eb), [prevent the spread of misinformation](https://cs.stanford.edu/~srijan/hoax/) in social networks (and probably also help it too), [detect fraud](https://towardsdatascience.com/fraud-detection-with-graph-analytics-2678e817b69e) , and [extract information from documents](https://medium.com/mlearning-ai/what-is-ai-document-understanding-f32da5f12055).^[*See* @STOKES2020688 for treatment of drug discovery, @kumar2016disinformation for misinformation in social networks, @10.1145/3447548.3467142 for fraud detection, and @doc2graph for document information extraction.]

As data scientists and machine learners, our main goal is to make sense of the world around us. We do some of that by building models in and through which our understanding may grow. When we begin to see graph structure and its natural projections onto the world, we unlock a deeper and more organic connection by which to explain and infer phenomena. Graphs unlock this connection by blending topological and geometric perspectives.

<br>

## Adding Geometry to Graphs

For machines to learn on data, be it graph-structured or not, we must represent it in a way computers can digest. That is to say, we must encode the data into some *n*-dimensional Euclidean vector space where individual data points become vectors within that space.

Consider GPS telemetry data used to monitor the movement of a newly released California Condor.^[Picture provided by James Sheppard. *See* https://blogs.biomedcentral.com/bmcseriesblog/2015/09/02/behind-image-3d-home-range-california-condor/.] 

![](images/condor_3d_map.jpeg)

The data used to make that visualization might look something like this:

```{r encoded condor data, echo=FALSE, message=FALSE, warning=FALSE}
# Load data
encoded_condor_data <- readRDS("data/encoded_condor_telemetry.RDs")

# Plot table
knitr::kable(
   encoded_condor_data,
   format = "html",
   row.names = FALSE,
   align = c("l", "c", "c", "c", "c"),
   escape = FALSE
   ) %>% 
   add_header_above(
      c("Condor GPS Telemetry Data" = 5),
      align = "c",
      escape = TRUE,
      line = FALSE,
      extra_css = "background-color: rgb(232, 215, 195); font-size: 1rem; padding: 0.15rem 0;"
      ) %>%
  footnote(
      general_title = "Notes:",
      title_format = "bold",
      general =
        "--- The Time column is normalized from 0 to 1 which represent the start and end of the tracking period, respectively. <br>
         --- Each condor has an identification number. Condor 190 is  <a href='https://www.ventanaws.org/condor190.html'>Redwood Queen</a> and Condor 1031 is one of her offspring,  <a href='https://www.ventanaws.org/condor1031.html'>Iniko</a>. Redwood Queen and Iniko are real California Condors which are part of the free-ranging Central California flock! They have an amazing <a href='https://www.wildtones.com/birding-info/the-story-of-condor-chick-iniko'>story</a>.",
      escape = FALSE
  )
```


We want to represent each row in the table as a vector.^[If we have data that is not represented with numbers, than there are [feature encoding](https://medium.com/analytics-vidhya/different-type-of-feature-engineering-encoding-techniques-for-categorical-variable-encoding-214363a016fb) processes which are used to turn categorical or lexical data, for example, into real-numbers such that they can still be digested by the computer as numeric vectors.] However, the Latitude and Longitude coordinate system is part of a fundamentally [different type of geometry](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/): a non-Euclidean one native to the 3-dimensional ellipsoidal shape of the Earth. We can convert this non-Euclidean space to a 2-d Euclidean space in the traditional Cartesian coordinate system by using the [Universal Transverse Mercator (UTM)](https://openpress.usask.ca/introgeomatics/chapter/cartesianprojected-coordinate-systems-utm/) map projection.^[Map projections are not perfect, and some distortion will occur depending on the reference point of the mapping.] The UTM projection gives us an $(easting, \ northing)$ point for each point in $(latitude, \ longitude)$ space. 

After we have performed the mapping all of our data can be represented in a 5-d Euclidean vector space where each vector fully describes a row in our data.

$$ \begin{bmatrix} Time \\ Condor \\ Easting \\ Northing \\ Altitude \end{bmatrix} \in \mathbb{R}^5  $$

This process introduces a consistent *geometry* to our data. That is, we can now understand the space our data exists within through its shape, size, distance, and relative position. Below, I visualize the Condor GPS Telemetry vector space, using some liberties to represent the *Time* and *Condor* dimensions as hue and point-shape, respectively.

```{r condor vector space, echo=FALSE, message=FALSE, warning=FALSE}
# Convert condor column to factor for plotly
encoded_condor_data$Condor <- as.factor(encoded_condor_data$Condor)
setDT(encoded_condor_data)

# Convert Lat, Long to UTM
coord_latlon <- SpatialPoints(
  cbind(encoded_condor_data$Longitude, encoded_condor_data$Latitude),
  proj4string = CRS("+proj=longlat")
  )

# Create data table and convert from meters to kilometers
coord_UTM <- data.table(spTransform(coord_latlon, CRS("+init=epsg:32748"))@coords) / 1000
setnames(coord_UTM, old = c("coords.x1", "coords.x2"), new = c("Easting", "Northing"))

# Combine tables 
condor_vector_space <- cbind(
  encoded_condor_data[, .(Time, Condor)],
  coord_UTM,
  encoded_condor_data[, .(Altitude)])


# Plot condor vector-space
fig <- plot_ly(
  condor_vector_space,
  type = 'scatter3d',
  mode = 'markers',
  x = ~Easting,
  y = ~Northing,
  z = ~Altitude,
  color = ~Time,
  colors = c("#bfd3e6", "#4d004b"),
  symbol = ~Condor,
  symbols = c("diamond", "circle")
  ) %>% layout(
    legend = list(title=list(text='Condor')),
    scene = list(
      xaxis = list(
        range = c(4730, 4731.5),
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        ),
      yaxis = list(
        range = c(24797, 24798.5),
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        ),
      zaxis = list(
        zerolinecolor = "#e8d8c3",
        gridwidth = 2,
        gridcolor = "#e8d8c3"
        )
    ),
    paper_bgcolor="#fff8e3",
    plot_bgcolor="#fff8e3"
  )

fig
```

Embedding our data into a vector space allows us to leverage Euclid's *Elements* and everything built upon them. We can compare how similar two vectors are by using methods like [cosine similarity](https://towardsdatascience.com/understanding-cosine-similarity-and-its-application-fd42f585296a) or feed these vectors into machine learning models as *features* to learn upon.

When our data has an inherent graph structure, the process is no different. We simply associate feature vectors with their given nodes or edges. Let's take our first graph from this post and re-imagine it as a representation of a small social network where nodes are user-profiles and edges indicate friendships between them. 

![](images/basic_graph-1-node-features.svg)

For each node we have added a feature vector which holds some information about those user-profiles such as how long they have been on the platform $x_1$, when they last logged on $x_2$, their geographic region $x_3$, and so on. 

Assigning feature vectors to the graph overlays a Euclidean geometry upon it. We can no longer move, bend, or switch nodes on the graph because each node now exists at a certain point in our feature-vector space. Moving the node would change its underlying meaning.

<br>

## The Intersection of Topology and Geometry

Graphs allow us to represent features alongside additional relationships and structure. Introducing a flexible topological structure in which Euclidean geometry can be embedded can help our models understand and generalize better.

Relational data is the most abundant type of data across industry, yet it is not often thought about in terms of its inherent graph structure.^[*See* @kaggle-survey-2021.] A simple database representation of our small social network below depicts the relational structure which can be explicitly embedded as the edges of a graph.

![](images/relational_db_example.png)

<figure>
    <figcaption>
    Each row in the user table (blue) holds information about a node in our graph while each row in the friendship table (green) represents an edge between the user who made the friend request and the user whom accepted. Each user can have many friendships (shown by colored lines).
    </figcaption>
</figure>

In this sense, all relational databases can be seen as graphs which allow for complex topologies through primary and foreign key relations like the one-to-many relationship we see above. The intuition behind adding this friendship-structure is that we can learn more about a user by also looking at who they are friends with and the attributes of those friends. 

An extremely common modelling task on relational data of social networks is predicting [user churn](https://medium.com/@malvikat/a-complete-guide-to-customer-churn-rate-fa6ec09d96d). We want to know which users are likely to drop off the platform and perhaps intervene. A tabular machine learning model might ingest the rows of the user table as features and learn to recognize patterns about certain types of users and their behavior which makes them likely to attrite. 

This may work relatively well, however, it ignores the underlying *topology of friendship* such users find themselves in. Consequently, we lose the ability to answer questions like: *if a user's friends churn, are they more likely to churn themselves?* The graph-based approach allows us to answer this question because we can capture, for each row in the data, additional information about relationships between rows.^[*See* https://opendatascience.com/machine-learning-with-graphs-going-beyond-tabular-data/.] @cvitkovic2020supervised shows incorporating the connectivity of relational data through graphs is especially useful when many tables and foreign key relationships belong to a relational database.^[Using this approach does not mean we must use graph-based models such as GNNs to perform the downstream task. We can use any graph-based embedding method to generate embedding which capture this additional structure. These embeddings can then  be used like any other embedding input into traditional tabular models.]

Graphs unlock this capability by incorporating topological structure as an inductive bias *alongside* features rather than as an additional feature. Acknowledging the inherent topology of our data helps models better understand *how to learn* rather than expanding *what to learn*. 

<br>

## Topology, All Along

Topological structure need not conform to the Euclidean feature-space. Graphs force us to understand this through their non-Euclidean connectivity. However, a different kind of topology has been lurking alongside us all along the way. 

When we think about the data most machine learning operates on such as tables, text, or images, we don't usually speak about topology. That is because these data types all enforce a strict *regular topology* that doesn't give us much signal to learn on. Images, for example, are represented as a regular grid of pixel values which can also be transformed into a graph.^[Figure from [Yalda Shankar's](https://yaldashankar.org) beautifully crafted visual-forward Graph ML overview: [The GNN Booklet](https://yaldashankar.org/demo/GNN-1.pdf).]

![](images/image_as_graph.png)

The structure of the pixel-grid will not change from image to image. That means the topology of the data can't help us understand what the image is of. All of the signal which differentiates images is held in the 2-d Euclidean pixel-space rather than the topology of the pixel-space. The same case can be made for text data in a 1-d space, or any *n*-dimensional Euclidean vector space (like that of our Condor GPS telemetry) because there is always regular grid structure.

Machine learning canon evolved using Euclidean data and enforced this regular topology implicitly. The invariances of the regular grid-structure were built into the models.^[*See* @DBLP:journals/corr/BronsteinBLSV16.] As an artifact, much of machine learning has bound model architecture and input structure. [Geometric Deep Learning](https://flawnsontong.medium.com/what-is-geometric-deep-learning-b2adb662d91d) is carving a different paradigm in which models are designed to understand and exploit the topological and geometric properties of their inputs. Models of this paradigm can adapt to different and more complex topology induced by graphs, [symmetry groups](https://www.cantorsparadise.com/an-invitation-to-group-theory-c81e21ab739a), and [manifolds](https://bjlkeng.github.io/posts/manifolds/) by allowing for dynamic computation graphs.^[*See* @NIPS2014_f9be311e for treatment of symmetry groups and @NIPS2017_0ebcc77d for treatment of manifolds.]

<br>

## To Blog Is To Forget

Looking back over the past year, I'd be remiss to not speak on the effects of entering into the fold through my own journey developing large-scale document AI systems. *Document AI* is a field which uses AI to comprehend documents such as [scanned invoices](https://guillaumejaume.github.io/FUNSD/), [webpages](https://x-lance.github.io/WebSRC/), [legal filings](https://data.nist.gov/pdr/lps/ark:/88434/mds2-2531), and in my experience: [receipts](https://paperswithcode.com/dataset/sroie).^[*See* @jaume2019 for more information on FUNSD, @DBLP:journals/corr/abs-2101-09465 for WebSRC, and @DBLP:journals/corr/abs-2103-10213 for SROIE.] It is broad and touches a wide range of AI specialties: CV, NLP, and even more recently Graph ML.^[Industrial application of document AI is equally broad, and includes, identifying types of documents (image classification), making document text searchable (optical character recognition), extracting specific pieces of text (key information extraction), and answering questions about documents (document question answering). *See* https://huggingface.co/blog/document-ai.]

Once brought into the fold of Graph ML, it can be hard to stop seeing graphs everywhere you look and especially hard to not send your CV and NLP colleagues blogs about how [CNNs](https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d) and [transformers](https://thegradient.pub/transformers-are-graph-neural-networks) are just special cases of graph neural networks. Yes, pretty much everything is a graph or can be molded into one, but doing so isn't always useful. Ignoring topology can help to not distract models (and the people who build them) when it lacks useful information. It also allows for highly-optimized and -parallelizable architectures rather than ones which are more flexible but have to unfurl computation graphs dynamically.

Through sheer luck and an amazing group of colleagues and mentors, my past year led me to explore Graph ML and the Geometric Deep Learning paradigm to search for and uncover a hidden *semantic topology* within the documents we process. While I can't say what this means for the future, I can say I have grown more than I could have imagined and I'm excited to see what 2023 holds.

<br>

As for Ptolemy, I've found another [answer](https://ai.stackexchange.com/questions/11226/what-is-non-euclidean-data/11259#11259). The shortest road between two points isn't always a straight line. . .

<br>

***
## References
